# LITE

## 1. Data Generate

Run **scripts/bo_sample.py** to automatically run the workload in spark-bench to generate log files.

## 2. Process Original Log

**scripts/history_by_stage.py** is used to parse the original log file generated by spark-bench, which get the running time of each stage, the amount of data read and write, etc.

After the above steps, the generated data is still based on workload, and you need to use **scripts/build_dataset.py** to divide each row into multiple stages. Then merge the data of all stages of all workloads into a complete data set and store it in csv format. The data set can be divided into training set and test set as needed.

## 3.Data Preprocessing

Obtain the stage code characteristics: enter the instrumentation folder, mark maven into a jar package, and add it to the spark-submit command.

```sh
--conf "spark.driver.extraJavaOptions=-javaagent:/pathtoyourjar/preMain-1.0.jar"
```

Our model is saved in prediction_nn,

You should use **data_process_text.py、dag2data.py、dataset_process.py** to get dictionary information、Process the edge and node information of the graph, and convert the data features into graph features.

## 4.Model Training

Then use **fast_train.py** to train model.

Use **trans_learn.py** finetune the model.

## 5.Model Testing

**nn_pred_8.py** can test the model，we use predict_first_cold.py to predict the best combination of parameters and check its actual operation.

